# -*- coding: utf-8 -*-
"""BUILDING_SYSTEMS_COLAB_PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YsKvQZKO1rUlwQrN41IoPA2pZw4JORXg

## Importing modules

## BUILDING SYSTEMS DATA DRIVE PROJECT
### OCCUPANCY PREDICTION 
  This project aims to predict the occupancy of an office room. The data used for this project was taken here: 
https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+ 
  
  Experimental data used for binary classification (room occupancy) from Temperature, Humidity, Light and CO2. 
  Ground-truth occupancy was obtained from time stamped pictures that were taken every minute.

Attribute Information:

*   date time: year-month-day hour:minute:second;
*   Temperature: Celsius;
*   Relative Humidity: %;
*   Light: Lux;
*   CO2: ppm;
*   Humidity Ratio: kgwater-vapor/kg-air;
*   Occupancy: 0 or 1, 0 for not occupied, 1 for occupied status.

### Installing PyDrive
"""

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)


# After a specific amount of time Pydrive logs out, the enxt function checks for that and if it has happend trys log in again
# you will not to log in to your account just the first time, the next times it will be automatic, we will use this function later on.
def authorizeIfLoggedOut():
  try:
    file_list_GDrive = drive.ListFile({'q': "'root' in parents and trashed=false"}).GetList()

  except:    
    auth.authenticate_user()
    gauth = GoogleAuth()
    gauth.credentials = GoogleCredentials.get_application_default()
    drive = GoogleDrive(gauth)

"""## Importing modules"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

print(drive.ListFile({'q': "'root' in parents and trashed=false"}).GetList())

# this will just check if we are logged out it logs in
authorizeIfLoggedOut()

file_list_GDrive = drive.ListFile({'q': "'root' in parents and trashed=false"}).GetList()

for file1 in file_list_GDrive:  
    if "Buildings_Project" in file1['title']:
        Folder_ID_Buildings_Project= file1['id']


print(Folder_ID_Buildings_Project)

file_list_Buildings_Project = drive.ListFile({'q': "'%s' in parents and trashed=false" % Folder_ID_Buildings_Project}).GetList()

for file2 in file_list_Buildings_Project:  
    if "TOTAL_DATA.csv" in file2['title']:
        print ("TOTAL_DATA.csv exists")
        ID_TOTAL_DATA= file2['id']
        file_TOTAL_DATA = drive.CreateFile({'id': ID_TOTAL_DATA})
        file_TOTAL_DATA.GetContentFile('TOTAL_DATA.csv')

"""## Importing datasets"""

DATA_DF_File = "TOTAL_DATA.csv"

DATA_DF = pd.read_csv(DATA_DF_File, sep = ",")
DATA_DF

"""## Creating time stamps
The program doesn't know yet that the index 'date' means a time related feature, so we make it into one:
"""

DATA_DF_Old_Index = DATA_DF["Time_Stamp"]
DATA_DF_newIndex = pd.to_datetime(DATA_DF_Old_Index) 

# DATA_DF_newIndex
#then we need to insert the new time index into our table

DATA_DF["Time_Stamp"] = DATA_DF_newIndex

DATA_DF

"""## Classification: Neural Network
From the features we'll try to classify wether the room is occupied or not.

### IMPORT MACHINE LEARNING LIBRARIES
Keras
"""

from keras import models
from keras import layers
from keras.callbacks import ModelCheckpoint   #how the model keeps the important data and discards unimportant data

"""First we will ignore the Time_Stamp feature, because for now we are not trying to predict, we are trying to classify. And also we disregard Humidity_Ratio because we already have a Humidity feature."""

DATA_DF.pop("HumidityRatio")

New_DATA_DF = DATA_DF

New_DATA_DF

"""We shuffle the New_DATA_DF in order to our network not to get used to a specific order."""

New_DATA_DF = New_DATA_DF.sample(frac=1).reset_index(drop = True)

New_DATA_DF

"""Data Normalization - avoid over/underfitting due to heterogenic data"""

features = ["Temperature", "Humidity", "Light", "CO2"]

for i in features:
  New_DATA_DF[i] = (New_DATA_DF[i] - New_DATA_DF[i].mean()) / New_DATA_DF[i].std()

New_DATA_DF

"""List of lists separating each randomized line"""

x = [[New_DATA_DF["Temperature"][i], New_DATA_DF["Humidity"][i], New_DATA_DF["Light"][i], New_DATA_DF["CO2"][i]] for i in range(len(New_DATA_DF))]

# for loop for the range of the data frame with the length of the New_DATA_DF in order to access the i-th value in each column

y = [[New_DATA_DF["Occupancy"][i]] for i in range(len(New_DATA_DF))]

# x - each array is a line in x

x = np.array(x, dtype = np.dtype("float32"))

y = np.array(y, dtype = np.dtype("int32")) #occupancy are values where 1 is occupied and 0 is empty.

"""## Training and Testing Data Separation
We separate the data into two categories: Testing and Training. The larger the training size the more accurate our classification.
"""

# len(New_DATA_DF)

training = 0.8*len(New_DATA_DF) # 80% for training 

training = int(training)
testing = len(New_DATA_DF)-training  # 20% for testing


x_training = x[:training]   #gets the numbers from start till the length of training
x_testing = x[training:]   #gets the numbers from training till end (length of testing)
#x_training

y_training = y[:training] 
y_testing = y[training:]

"""## CHECKPOINT
Our Neural Network now will be able to save the best training Epochs. Epochs are each time our Network ran and tested all the data. Everytime the Network tests the data, it checks wether the test was accurate or not, and then we tell it to keep the most accurate ones using the checkpoints.
"""

checkpoint = ModelCheckpoint("./weights.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

# ./weights.h5 -> this is the kind of file we are saving, called "weights"
# monitor = 'val_accuracy' -> parameter to monitor - validation and accuracy
# verose = 1 -> to tell me it saved the file

model = models.Sequential() # type of model - sequential model
model.add(layers.Dense(32, activation="relu", input_shape=(4,))) 
model.add(layers.Dense(32, activation="relu"))
model.add(layers.Dense(1, activation="sigmoid"))
model.compile(optimizer="rmsprop", loss="binary_crossentropy", metrics=["accuracy"]) #compile the module - optimize function that modifies the tests (weights) each time

# each layer has Neurons that are connected to each other and each has an activation function (relu = rectified linear units, sigmoid = bc its binary)
# there are 3 layers, 1 - input, 2 - hidden (makes the tests), 3 - output

"""## LET'S TRAIN
We call the training function "model.fit" in order to train our New Neural Network. The ouput "history" is going to show us how accurate our training is for each Epoch.

Overfitting - if the code gets too used to the test_data and doesn't generalize the features well, it will give a good performance on the training_data, but the validation_data will not show a good performance (exemple: data that it never seen before)

Underfitting - the code can't learn the relation between the features and the label, which results in very low accuracy values.
"""

history = model.fit(x_training, y_training, epochs=100, batch_size=128, validation_data=(x_testing, y_testing), callbacks=[checkpoint])

"""**Epoch 100/100**
16448/16448 [==============================] - 0s 12us/step - loss: 0.0304 - acc: 0.9904 - val_loss: 0.0414 - 
**val_acc: 0.9903**

## Plotting results
"""

accuracy = history.history["acc"]
# inside 'history' we have a dictionary that gives us values for each Epoch test and we can plot their progression from Epoch 1 to 1000
# accuracy is the training part that was tested on the epoch and compared to the real value

val_accuracy = history.history["val_acc"]
# validation is the testing array that our network never seen before (outside the training) that will be what really checks the performance of our network 
# in the real case.

plt.figure(figsize=(10,6))
plt.grid()
plt.plot(accuracy, "blue")
plt.plot(val_accuracy, "green")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.show()

"""## OCCUPANCY CLASSIFICATION VERIFICATION:"""

# DATA_DF.loc["2015-02-02 14:19:00"]

def get_index_by_timestamp(df, time_stamp):
    index = DATA_DF[DATA_DF["Time_Stamp"] == time_stamp].index[0]
    return index

def predict_occupancy(df, index):

    test_x = np.array([[ df["Temperature"][index], df["Humidity"][index], df["Light"][index], df["CO2"][index]]])
    prediction = model.predict_classes(test_x)[0][0]
    if prediction == 1 : 
      print("Neural Network Prediction: Room is occupied")
    else : 
      print("Neural Network Prediction: Room is empty")

    real_occupancy = df["Occupancy"][index]
    if real_occupancy == 1:
      print ("Real occupancy: The room is occupied")
    else:
      print ("Real occupancy: room is empty")

index = get_index_by_timestamp(DATA_DF,"2015-02-18 09:03:00")

predict_occupancy(DATA_DF,index)

"""## OCCUPANCY 2-WEEKS PREDICTION
The objective of this part of the code is to now make a prediction for the next two weeks regarding the occupancy of the room.

We need to import linear regression modes from "sklearn" to create a new Neural Network able to give us predictions.

## LINEAR REGRESSION - SPLIT
"""

# we need to work with values that aren't strings so we need to make the time_stamp into a float
from datetime import datetime
def convert_to_stamp(time):
    timestamp = datetime.timestamp(time)
    return timestamp

DATA_DF.loc[:,"Time_Stamp_Float"] = [[convert_to_stamp(New_DATA_DF["Time_Stamp"][i])] for i in range(len(New_DATA_DF["Time_Stamp"]))]
#DATA_DF

DATA_DF_target = DATA_DF.loc[:,["Occupancy"]] #this way we have a data_frame

DATA_DF_features = DATA_DF.drop(columns= ["Time_Stamp","Occupancy"]) #we use as features everything except the string and our target

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(DATA_DF_features,DATA_DF_target, test_size = 0.2, random_state = 41234)

X_train

from sklearn import linear_model

linear_reg = linear_model.LinearRegression()
# linear_reg

linear_reg.fit(X_train,Y_train)

predicted_linearReg_split = linear_reg.predict(X_test)
# predicted_linearReg_split

predicted_DF_linearReg_split = pd.DataFrame(predicted_linearReg_split, index = Y_test.index, columns = ["Predicted_Occupancy"])
predicted_DF_linearReg_split

#it will give the prediction of AC consumption on a specific date
predicted_DF_linearReg_split = predicted_DF_linearReg_split.join(Y_test) #you add the output to the table

predicted_DF_linearReg_split[predicted_DF_linearReg_split["Predicted_Occupancy"] <= 0] = 0
predicted_DF_linearReg_split[predicted_DF_linearReg_split["Predicted_Occupancy"] >= 1] = 1

Time = DATA_DF.loc[predicted_DF_linearReg_split.index,"Time_Stamp"]
predicted_DF_linearReg_split.insert(0, "Time_Stamp", Time)

predicted_DF_linearReg_split

predicted =[]
actual = []
for index,row in predicted_DF_linearReg_split.iterrows():
  predicted.append( row["Predicted_Occupancy"])
  actual.append( row["Occupancy"])

plt.figure(figsize = (20,12))
plt.title("Comparison between linear regression prediction and actual value")
plt.scatter(range(len(predicted)), predicted, c="r") 
plt.scatter(range(len(actual)), actual)
plt.xlabel("length")
plt.ylabel("Occupancy")
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

MAE_linearReg_split = mean_absolute_error(predicted_linearReg_split, Y_test)
MSE_linearReg_split = mean_squared_error(predicted_linearReg_split, Y_test)
Mr2_linearReg_split = r2_score(predicted_linearReg_split, Y_test)

print ("MAE :" +str(MAE_linearReg_split))
print ("MSE :" +str(MSE_linearReg_split))
print("Mr2 :" +str(Mr2_linearReg_split))

"""## CROSS VALIDATION"""

from sklearn.model_selection import cross_val_predict

predicted_linearReg_CV = cross_val_predict(linear_reg,DATA_DF_features,DATA_DF_target, cv = 1200)

predicted_DF_linearReg_CV = pd.DataFrame(predicted_linearReg_CV, index = DATA_DF_target.index, columns = ["Predicted_Occupancy"])

#it will give the prediction of AC consumption on a specific date for 300 rows at a time
predicted_DF_linearReg_CV = predicted_DF_linearReg_CV.join(DATA_DF_target) #you add the output to the table

predicted_DF_linearReg_CV[predicted_DF_linearReg_CV["Predicted_Occupancy"] <= 0] = 0
predicted_DF_linearReg_CV[predicted_DF_linearReg_CV["Predicted_Occupancy"] >= 1] = 1

Time_CV = DATA_DF.loc[predicted_DF_linearReg_CV.index,"Time_Stamp"]
predicted_DF_linearReg_CV.insert(0, "Time_Stamp", Time_CV)


predicted_DF_linearReg_CV[predicted_DF_linearReg_CV["Occupancy"] == 0]

predicted =[]
actual = []
for index,row in predicted_DF_linearReg_CV.iterrows():
  predicted.append( row["Predicted_Occupancy"])
  actual.append( row["Occupancy"])

plt.figure(figsize = (15,8))
plt.title("Comparison between linear regression prediction and actual value")
plt.scatter(range(len(predicted)), predicted, c="r") 
plt.scatter(range(len(actual)), actual)
plt.xlabel("length")
plt.ylabel("Occupancy")
plt.show()

MAE_linearReg_CV = mean_absolute_error(predicted_linearReg_CV, DATA_DF_target)
MSE_linearReg_CV = mean_squared_error(predicted_linearReg_CV, DATA_DF_target)
Mr2_linearReg_CV = r2_score(predicted_linearReg_CV, DATA_DF_target)

print ("MAE_CV :" +str(MAE_linearReg_CV))
print ("MSE_CV :" +str(MSE_linearReg_CV))
print("Mr2_CV :" +str(Mr2_linearReg_CV))

"""## RANDOM FORESTS"""

from sklearn.ensemble import RandomForestRegressor

RF_reg = RandomForestRegressor()

predicted_RF_reg_CV = cross_val_predict(RF_reg,DATA_DF_features,DATA_DF_target, cv = 300)

predicted_DF_RF_reg_CV = pd.DataFrame(predicted_RF_reg_CV, index = DATA_DF_target.index, columns = ["Predicted_Occupancy"])

#it will give the prediction of AC consumption on a specific date for 10 rows at a time
predicted_DF_RF_reg_CV = predicted_DF_RF_reg_CV.join(DATA_DF_target) #you add the output to the table

Time_RF = DATA_DF.loc[predicted_DF_RF_reg_CV.index,"Time_Stamp"]
predicted_DF_RF_reg_CV.insert(0, "Time_Stamp", Time_RF)

predicted_DF_RF_reg_CV[predicted_DF_RF_reg_CV["Occupancy"] == 0]

predicted =[]
actual = []
for index,row in predicted_DF_RF_reg_CV.iterrows():
  predicted.append( row["Predicted_Occupancy"])
  actual.append( row["Occupancy"])

plt.figure(figsize = (15,8))
plt.title("Comparison between linear regression prediction and actual value")
plt.scatter(range(len(predicted)), predicted, c="r") 
plt.scatter(range(len(actual)), actual)
plt.xlabel("length")
plt.ylabel("Occupancy")
plt.show()

MAE_RF_reg_CV = mean_absolute_error(predicted_RF_reg_CV, DATA_DF_target)
MSE_RF_reg_CV = mean_squared_error(predicted_RF_reg_CV, DATA_DF_target)
Mr2_RF_reg_CV = r2_score(predicted_RF_reg_CV, DATA_DF_target)

print ("MAE_RF :" +str(MAE_RF_reg_CV))
print ("MSE_RF :" +str(MSE_RF_reg_CV))
print("Mr2_RF :" +str(Mr2_RF_reg_CV))

plt.figure(figsize = (10,6))
plt.ylabel("Accuracy")
sns.barplot(x = ["Mr2_SPLIT","Mr2_RF", "Mr2_CV"] , y = [Mr2_linearReg_split, Mr2_RF_reg_CV, Mr2_linearReg_CV])
plt.show()